{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA LOADING\n",
    "with open(\"Project 1 Materials\\proj1_data.json\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMARY VARIABLES\n",
    "X = []\n",
    "y = []\n",
    "training_X = []\n",
    "training_Y = []\n",
    "cross_validation_X = []\n",
    "cross_validation_Y = []\n",
    "testing_X = []\n",
    "testing_Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PREPROCESSING\n",
    "\n",
    "#Splits a given text into individual tokens\n",
    "def splitText(text):\n",
    "    lowerCaseText =  text.lower()\n",
    "    splitText = lowerCaseText.split()\n",
    "    return splitText\n",
    "\n",
    "#Find top 160 frequently occurring word in 'data'\n",
    "def topFrequencies(dictionaryList, num):\n",
    "    countVariable = Counter()\n",
    "    for item in dictionaryList:\n",
    "        splitted_text = splitText(item['text'])\n",
    "        for word in splitted_text:\n",
    "            countVariable[word] += 1\n",
    "    return dict(countVariable.most_common(num))\n",
    "\n",
    "#Convert list of key-value pairs (word, frequency) to (word, ranking)\n",
    "def convertFrequenciesToRankings(list):\n",
    "    i = 0\n",
    "    newList = {}\n",
    "    for word in list:\n",
    "        newList[word] = i\n",
    "        i+=1\n",
    "    return newList\n",
    "\n",
    "#Construct a 160 word count vector from a comment\n",
    "def construct160WordCountVector(text):\n",
    "    vector = [0]*160\n",
    "    splitted_text = splitText(text)\n",
    "    for word in splitted_text:\n",
    "        i = rankingOfMostFrequent160Words.get(word, -1)\n",
    "        if (i != -1): vector[i] += 1\n",
    "    return vector\n",
    "\n",
    "#Construct a 60 word count vector from a comment\n",
    "def construct60WordCountVector(text):\n",
    "    vector = [0]*60\n",
    "    splitted_text = splitText(text)\n",
    "    for word in splitted_text:\n",
    "        i = rankingOfMostFrequent60Words.get(word, -1)\n",
    "        if (i != -1): vector[i] += 1\n",
    "    return vector\n",
    "\n",
    "mostFrequent160Words = topFrequencies(data, 160)\n",
    "rankingOfMostFrequent160Words = convertFrequenciesToRankings(mostFrequent160Words)\n",
    "\n",
    "mostFrequent60Words = topFrequencies(data, 60)\n",
    "rankingOfMostFrequent60Words = convertFrequenciesToRankings(mostFrequent60Words)\n",
    "\n",
    "\n",
    "#Extra feature 1: Number of words per comment\n",
    "#Count the number of words in a comment\n",
    "def countWord(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    return num_words\n",
    "    \n",
    "\n",
    "#Extra feature 2: Average length of all words in a comment\n",
    "#Calculate the average length of words in a comment\n",
    "def averageWordLength(text):\n",
    "    words = text.split()\n",
    "    numWords = len(words)\n",
    "    totalNumLetters = len(text) - text.count(' ')\n",
    "    avgNumLetters = totalNumLetters/numWords\n",
    "    return avgNumLetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARD DATASET CONSTRUCTION FUNCTIONS\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 160 word count feature\n",
    "def construct_dataset_1(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct160WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 60 word count feature\n",
    "def construct_dataset_2(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct60WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET CONSTRUCTION FUNCTIONS WITH EXTRA FEATURES\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 160 word count feature + 2 additional features\n",
    "def construct_dataset_3(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        \n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct160WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "                \n",
    "                wordCount = countWord(value)\n",
    "                training_example.append(wordCount)\n",
    "                \n",
    "                avgWordLength = averageWordLength(value)\n",
    "                training_example.append(avgWordLength)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 60 word count feature + 2 additional features\n",
    "def construct_dataset_4(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct60WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "                                \n",
    "                wordCount = countWord(value)\n",
    "                training_example.append(wordCount)\n",
    "                \n",
    "                avgWordLength = averageWordLength(value)\n",
    "                training_example.append(avgWordLength)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET CONSTRUCTION FUNCTIONS WITH BASIC PRESCRIBED FEATURES (EXCLUDES TEXT PROCESSING)\n",
    "\n",
    "#Constructs a dataset with the prescribed features excluding text features\n",
    "def construct_dataset_5(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key != \"text\"):\n",
    "                if (key == \"popularity_score\"):\n",
    "                    y.append(value)\n",
    "                elif (key == \"is_root\"):\n",
    "                    if (value == True): training_example.append(1)\n",
    "                    elif (value == False): training_example.append(0)\n",
    "                else: training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y\n",
    "\n",
    "#Constructs a dataset with the prescribed features + 2 extra features but excluding text features\n",
    "def construct_dataset_6(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                #POSSIBLE TEXT FEATURE\n",
    "                wordCount = countWord(value)\n",
    "                training_example.append(wordCount)\n",
    "                \n",
    "                #POSSIBLE TEXT FEATURE\n",
    "                avgWordLength = averageWordLength(value)\n",
    "                training_example.append(avgWordLength)\n",
    "                \n",
    "                #POSSIBLE TEXT FEATURE\n",
    "                #interactionFeature = math.log(wordCount * p['children']  + 1, 2)\n",
    "                #training_example.append(interactionFeature)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET SEPARATION\n",
    "def split_dataset(X,y):\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    cv_X = []\n",
    "    cv_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(10000):\n",
    "        train_X.append(X[i])\n",
    "        train_Y.append(y[i])\n",
    "    for i in range(1000):\n",
    "        cv_X.append(X[i + 10000])\n",
    "        cv_Y.append(y[i + 10000])\n",
    "    for i in range(1000):\n",
    "        test_X.append(X[i + 11000])\n",
    "        test_Y.append(y[i + 11000])\n",
    "    return train_X, train_Y, cv_X, cv_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 Constructed\n",
      "\n",
      "Dataset 1 Split\n",
      "\n",
      "Dataset 1 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 1\n",
    "X, y = construct_dataset_1(data)\n",
    "print(\"Dataset 1 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 1 Split\\n\")\n",
    "np.save(\"training_data_X_1.npy\", training_X)\n",
    "np.save(\"training_data_Y_1.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_1.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_1.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_1.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_1.npy\", testing_Y)\n",
    "print(\"Dataset 1 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 Constructed\n",
      "\n",
      "Dataset 2 Split\n",
      "\n",
      "Dataset 2 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 2\n",
    "X, y = construct_dataset_2(data)\n",
    "print(\"Dataset 2 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 2 Split\\n\")\n",
    "np.save(\"training_data_X_2.npy\", training_X)\n",
    "np.save(\"training_data_Y_2.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_2.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_2.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_2.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_2.npy\", testing_Y)\n",
    "print(\"Dataset 2 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3 Constructed\n",
      "\n",
      "Dataset 3 Split\n",
      "\n",
      "Dataset 3 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 3\n",
    "X, y = construct_dataset_3(data)\n",
    "print(\"Dataset 3 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 3 Split\\n\")\n",
    "np.save(\"training_data_X_3.npy\", training_X)\n",
    "np.save(\"training_data_Y_3.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_3.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_3.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_3.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_3.npy\", testing_Y)\n",
    "print(\"Dataset 3 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 4 Constructed\n",
      "\n",
      "Dataset 4 Split\n",
      "\n",
      "Dataset 4 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 4\n",
    "X, y = construct_dataset_4(data)\n",
    "print(\"Dataset 4 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 4 Split\\n\")\n",
    "np.save(\"training_data_X_4.npy\", training_X)\n",
    "np.save(\"training_data_Y_4.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_4.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_4.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_4.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_4.npy\", testing_Y)\n",
    "print(\"Dataset 4 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 5 Constructed\n",
      "\n",
      "Dataset 5 Split\n",
      "\n",
      "Dataset 5 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 5\n",
    "X, y = construct_dataset_5(data)\n",
    "print(\"Dataset 5 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 5 Split\\n\")\n",
    "np.save(\"training_data_X_5.npy\", training_X)\n",
    "np.save(\"training_data_Y_5.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_5.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_5.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_5.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_5.npy\", testing_Y)\n",
    "print(\"Dataset 5 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 6 Constructed\n",
      "\n",
      "Dataset 6 Split\n",
      "\n",
      "Dataset 6 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 6\n",
    "X, y = construct_dataset_6(data)\n",
    "print(\"Dataset 6 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 6 Split\\n\")\n",
    "np.save(\"training_data_X_6.npy\", training_X)\n",
    "np.save(\"training_data_Y_6.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_6.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_6.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_6.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_6.npy\", testing_Y)\n",
    "print(\"Dataset 6 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'is_root' features that are true =  248\n",
      "Number of 'is_root' features that are false =  9752\n",
      "Number of 'controversy' features that are true =  4207\n",
      "Number of 'controversy' features that are false =  5793\n",
      "Average word count =  24.004\n",
      "Average word length =  5.288804888570435\n",
      "Average is root =  0.4207\n",
      "Average controversy =  0.0117\n",
      "Average children =  0.4024\n"
     ]
    }
   ],
   "source": [
    "#COUNTING DISPARITY OF BOOLEAN FEATURES AND AVERAGES OF SCALAR FEATURES\n",
    "A = np.load(\"training_data_X_2.npy\")\n",
    "numRTrue = 0   #Count for isRoot = True\n",
    "numRFalse = 0  #Count for isRoot = False\n",
    "numCTrue = 0   #Count for Controversiality = True\n",
    "numCFalse = 0  #Count for Controversiality = False\n",
    "i = 0\n",
    "for line in A:\n",
    "    if (line[60] == 1): numRTrue+=1\n",
    "    else: numRFalse += 1\n",
    "    if (line[61] == 1): numCTrue+=1\n",
    "    else: numCFalse += 1\n",
    "    i+=1\n",
    "print(\"Number of 'is_root' features that are true = \", numRTrue)\n",
    "print(\"Number of 'is_root' features that are false = \", numRFalse)\n",
    "print(\"Number of 'controversy' features that are true = \", numCTrue)\n",
    "print(\"Number of 'controversy' features that are false = \", numCFalse)\n",
    "\n",
    "A = np.load(\"training_data_X_3.npy\")\n",
    "avgWordCount = 0\n",
    "avgWordLength = 0\n",
    "avgControversy = 0\n",
    "avgIsRoot = 0\n",
    "avgChildren = 0\n",
    "counter = 0\n",
    "for x in A:\n",
    "    counter += 1\n",
    "    avgWordCount += x[161]\n",
    "    avgWordLength += x[162]\n",
    "    avgIsRoot += x[163]\n",
    "    avgControversy += x[164]\n",
    "    avgChildren += x[165]\n",
    "avgWordCount = avgWordCount/counter\n",
    "avgWordLength = avgWordLength/counter\n",
    "avgIsRoot = avgIsRoot/counter\n",
    "avgControversy = avgControversy/counter\n",
    "avgChildren = avgChildren/counter\n",
    "\n",
    "print(\"Average word count = \", avgWordCount)\n",
    "print(\"Average word length = \", avgWordLength)\n",
    "print(\"Average is root = \", avgIsRoot)\n",
    "print(\"Average controversy = \", avgControversy)\n",
    "print(\"Average children = \", avgChildren)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
