{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA LOADING\n",
    "with open(\"Project 1 Materials\\proj1_data.json\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMARY VARIABLES\n",
    "X = []\n",
    "y = []\n",
    "training_X = []\n",
    "training_Y = []\n",
    "cross_validation_X = []\n",
    "cross_validation_Y = []\n",
    "testing_X = []\n",
    "testing_Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PREPROCESSING\n",
    "\n",
    "#Splits a given text into individual tokens\n",
    "def splitText(text):\n",
    "    lowerCaseText =  text.lower()\n",
    "    splitText = lowerCaseText.split()\n",
    "    return splitText\n",
    "\n",
    "#Find top 160 frequently occurring word in 'data'\n",
    "def topFrequencies(dictionaryList, num):\n",
    "    countVariable = Counter()\n",
    "    for item in dictionaryList:\n",
    "        splitted_text = splitText(item['text'])\n",
    "        for word in splitted_text:\n",
    "            countVariable[word] += 1\n",
    "    return dict(countVariable.most_common(num))\n",
    "\n",
    "#Convert list of key-value pairs (word, frequency) to (word, ranking)\n",
    "def convertFrequenciesToRankings(list):\n",
    "    i = 0\n",
    "    newList = {}\n",
    "    for word in list:\n",
    "        newList[word] = i\n",
    "        i+=1\n",
    "    return newList\n",
    "\n",
    "#Construct a 160 word count vector from a comment\n",
    "def construct160WordCountVector(text):\n",
    "    vector = [0]*160\n",
    "    splitted_text = splitText(text)\n",
    "    for word in splitted_text:\n",
    "        i = rankingOfMostFrequent160Words.get(word, -1)\n",
    "        if (i != -1): vector[i] += 1\n",
    "    return vector\n",
    "\n",
    "#Construct a 60 word count vector from a comment\n",
    "def construct60WordCountVector(text):\n",
    "    vector = [0]*60\n",
    "    splitted_text = splitText(text)\n",
    "    for word in splitted_text:\n",
    "        i = rankingOfMostFrequent60Words.get(word, -1)\n",
    "        if (i != -1): vector[i] += 1\n",
    "    return vector\n",
    "\n",
    "mostFrequent160Words = topFrequencies(data, 160)\n",
    "rankingOfMostFrequent160Words = convertFrequenciesToRankings(mostFrequent160Words)\n",
    "\n",
    "mostFrequent60Words = topFrequencies(data, 60)\n",
    "rankingOfMostFrequent60Words = convertFrequenciesToRankings(mostFrequent60Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET CONSTRUCTION FUNCTIONS\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 160 word count feature\n",
    "def construct_dataset_1(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        \n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct160WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y\n",
    "\n",
    "#Constructs a dataset with the prescribed features + a 60 word count feature\n",
    "def construct_dataset_2(d):\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in d:\n",
    "        training_example = [1]\n",
    "        for key, value in p.items():\n",
    "            if (key == \"popularity_score\"):\n",
    "                y.append(value)\n",
    "            elif (key == \"is_root\"):\n",
    "                if (value == True): training_example.append(1)\n",
    "                elif (value == False): training_example.append(0)\n",
    "            elif (key == \"text\"):\n",
    "                vector = construct60WordCountVector(value)\n",
    "                training_example.extend(vector)\n",
    "            else:\n",
    "                training_example.append(value)\n",
    "        X.append(training_example)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET SEPARATION\n",
    "def split_dataset(X,y):\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    cv_X = []\n",
    "    cv_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(10000):\n",
    "        train_X.append(X[i])\n",
    "        train_Y.append(y[i])\n",
    "    for i in range(1000):\n",
    "        cv_X.append(X[i + 10000])\n",
    "        cv_Y.append(y[i + 10000])\n",
    "    for i in range(1000):\n",
    "        test_X.append(X[i + 11000])\n",
    "        test_Y.append(y[i + 11000])\n",
    "    return train_X, train_Y, cv_X, cv_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 Constructed\n",
      "\n",
      "Dataset 1 Split\n",
      "\n",
      "Dataset 1 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 1\n",
    "X, y = construct_dataset_1(data)\n",
    "print(\"Dataset 1 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 1 Split\\n\")\n",
    "np.save(\"training_data_X_1.npy\", training_X)\n",
    "np.save(\"training_data_Y_1.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_1.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_1.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_1.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_1.npy\", testing_Y)\n",
    "print(\"Dataset 1 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 Constructed\n",
      "\n",
      "Dataset 2 Split\n",
      "\n",
      "Dataset 2 Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERATE AND SAVE DATASET 2\n",
    "X, y = construct_dataset_2(data)\n",
    "print(\"Dataset 2 Constructed\\n\")\n",
    "training_X, training_Y, cross_validation_X, cross_validation_Y, testing_X, testing_Y = split_dataset(X,y)\n",
    "print(\"Dataset 2 Split\\n\")\n",
    "np.save(\"training_data_X_2.npy\", training_X)\n",
    "np.save(\"training_data_Y_2.npy\", training_Y)\n",
    "np.save(\"cross_validation_data_X_2.npy\", cross_validation_X)\n",
    "np.save(\"cross_validation_data_Y_2.npy\", cross_validation_Y)\n",
    "np.save(\"testing_data_X_2.npy\", testing_X)\n",
    "np.save(\"testing_data_Y_2.npy\", testing_Y)\n",
    "print(\"Dataset 2 Saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "11709\n",
      "12000\n",
      "4993\n",
      "7007\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "#COUNTING PROPORTIONS OF BOOLEAN FEATURES\n",
    "numRTrue = 0   #Count for isRoot = True\n",
    "numRFalse = 0  #Count for isRoot = False\n",
    "numCTrue = 0   #Count for Controversiality = True\n",
    "numCFalse = 0  #Count for Controversiality = False\n",
    "i = 0\n",
    "#Here X is based of dataset 2 since that was the last to be generated\n",
    "for line in X:\n",
    "    if (line[60] == 1): numRTrue+=1\n",
    "    else: numRFalse += 1\n",
    "    if (line[61] == 1): numCTrue+=1\n",
    "    else: numCFalse += 1\n",
    "    i+=1\n",
    "print(numRTrue)\n",
    "print(numRFalse)\n",
    "print(numRTrue + numRFalse)\n",
    "print(numCTrue)\n",
    "print(numCFalse)\n",
    "print(numCTrue + numCFalse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
